{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impossibility-global-convergence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK54L-xijTot",
        "colab_type": "text"
      },
      "source": [
        "Accompanying code for the paper: *On the Impossibility of Global Convergence in Differentiable Games*. Implements a number of multi-loss optimization methods that are shown to enter limit cycles instead of converging, for almost-all initialisations, even in a zero-sum game. This includes GD, [EG](https://arxiv.org/pdf/1906.05945.pdf), [OMD](https://arxiv.org/pdf/1711.00141.pdf), [CO](https://arxiv.org/pdf/1705.10461.pdf), [SGA](https://arxiv.org/pdf/1802.05642.pdf), [LA](https://openreview.net/pdf?id=SyGjjsC5tQ), [LOLA](https://arxiv.org/pdf/1709.04326.pdf), [SOS](https://openreview.net/pdf?id=SyGjjsC5tQ), [CGD](https://arxiv.org/pdf/1905.12103.pdf) and [LSS](https://arxiv.org/pdf/1901.00838.pdf). The notebook runs in ~5 minutes with GPU accelerator enabled in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCP-3QXijoq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-darkgrid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgmL9y6-6db_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown Gradient computations for each algorithm.\n",
        "\n",
        "def init_th(dims, std=1):\n",
        "  th = []\n",
        "  for i in range(len(dims)):\n",
        "    if std > 0:\n",
        "      init = torch.nn.init.normal_(torch.empty(dims[i], requires_grad=True), std=std)\n",
        "    else:\n",
        "      init = torch.zeros(dims[i], requires_grad=True)\n",
        "    th.append(init)\n",
        "  return th\n",
        "\n",
        "def get_gradient(function, param):\n",
        "  try:\n",
        "    grad = torch.autograd.grad(function, param, create_graph=True)[0]\n",
        "  except:\n",
        "    grad = torch.zeros(param.shape)\n",
        "  return grad\n",
        "\n",
        "def get_hessian(th, grad_L, diag=True, off_diag=True):\n",
        "  n = len(th)\n",
        "  H = []\n",
        "  for i in range(n):\n",
        "    row_block = []\n",
        "    for j in range(n):\n",
        "      if (i == j and diag) or (i != j and off_diag):\n",
        "        block = [torch.unsqueeze(get_gradient(grad_L[i][i][k], th[j]), dim=0) \n",
        "                  for k in range(len(th[i]))]\n",
        "        row_block.append(torch.cat(block, dim=0))\n",
        "      else:\n",
        "        row_block.append(torch.zeros(len(th[i]), len(th[j])))\n",
        "    H.append(torch.cat(row_block, dim=1))\n",
        "  return torch.cat(H, dim=0)\n",
        "\n",
        "def update_th(th, Ls, alpha, algo, sos_a=0.5, sos_b=0.5, co_gam=0.1, lss_t=0,\n",
        "              omd_th=None):\n",
        "  n = len(th)\n",
        "  losses = Ls(th)\n",
        "\n",
        "  # Compute gradients\n",
        "  grad_L = [[get_gradient(losses[j], th[i]) for j in range(n)] for i in range(n)]\n",
        "  if algo == 'la':\n",
        "    terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
        "                for j in range(n) if j != i]) for i in range(n)]\n",
        "    grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
        "  elif algo == 'lola':\n",
        "    terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j])\n",
        "                for j in range(n) if j != i]) for i in range(n)]\n",
        "    grads = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
        "  elif algo == 'sos':\n",
        "    terms = [sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
        "                for j in range(n) if j != i]) for i in range(n)]\n",
        "    xi_0 = [grad_L[i][i]-alpha*get_gradient(terms[i], th[i]) for i in range(n)]\n",
        "    chi = [get_gradient(sum([torch.dot(grad_L[j][i].detach(), grad_L[j][j])\n",
        "              for j in range(n) if j != i]), th[i]) for i in range(n)]\n",
        "    # Compute p\n",
        "    dot = torch.dot(-alpha*torch.cat(chi), torch.cat(xi_0))\n",
        "    p1 = 1 if dot >= 0 else min(1, -sos_a*torch.norm(torch.cat(xi_0))**2/dot)\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    xi_norm = torch.norm(xi)\n",
        "    p2 = xi_norm**2 if xi_norm < sos_b else 1\n",
        "    p = min(p1, p2)\n",
        "    grads = [xi_0[i]-p*alpha*chi[i] for i in range(n)]\n",
        "  elif algo == 'sga':\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    norm = torch.dot(xi, xi.detach())\n",
        "    H_t_xi = [get_gradient(norm, th[i]) for i in range(n)]\n",
        "    H_xi = [get_gradient(sum([torch.dot(grad_L[j][i], grad_L[j][j].detach())\n",
        "              for j in range(n)]), th[i]) for i in range(n)]\n",
        "    A_t_xi = [H_t_xi[i]/2-H_xi[i]/2 for i in range(n)]\n",
        "    # Compute lambda (sga with alignment)\n",
        "    dot_xi = torch.dot(xi, torch.cat(H_t_xi))\n",
        "    dot_A = torch.dot(torch.cat(A_t_xi), torch.cat(H_t_xi))\n",
        "    d = sum([len(th[i]) for i in range(n)])\n",
        "    lam = torch.sign(dot_xi*dot_A/d)\n",
        "    grads = [grad_L[i][i]+lam*A_t_xi[i] for i in range(n)]\n",
        "  elif algo == 'co':\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    norm = torch.dot(xi, xi.detach())\n",
        "    grads = [grad_L[i][i]+co_gam*get_gradient(norm, th[i]) for i in range(n)]\n",
        "  elif algo == 'eg':\n",
        "    th_eg = [th[i]-alpha*grad_L[i][i] for i in range(n)]\n",
        "    losses_eg = Ls(th_eg)\n",
        "    grads = [get_gradient(losses_eg[i], th_eg[i]) for i in range(n)]\n",
        "  elif algo == 'omd':\n",
        "    past_grad = [get_gradient(Ls(omd_th)[i], omd_th[i]) for i in range(n)]\n",
        "    grads = [2*grad_L[i][i]-past_grad[i] for i in range(n)]\n",
        "  elif algo == 'cgd': # Slow implementation (exact matrix inversion)\n",
        "    dims = [len(th[i]) for i in range(n)]\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    H_o = get_hessian(th, grad_L, diag=False)\n",
        "    grad = torch.matmul(torch.inverse(torch.eye(sum(dims))+alpha*H_o), xi)\n",
        "    grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
        "  elif algo == 'lss': # Slow implementation (exact matrix inversion)\n",
        "    dims = [len(th[i]) for i in range(n)]\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    H = get_hessian(th, grad_L)\n",
        "    if torch.det(H) == 0:\n",
        "      lam = 1-torch.exp(-torch.dot(xi, xi))\n",
        "      inv = torch.inverse(torch.matmul(H.t(), H)+lam*torch.eye(sum(dims)))\n",
        "      H_inv = torch.matmul(inv, H.t())\n",
        "    else:\n",
        "      H_inv = torch.inverse(H)\n",
        "    grad = torch.matmul(torch.eye(sum(dims))+torch.matmul(H.t(), H_inv), xi)/2\n",
        "    grad += (1-torch.exp(-torch.dot(xi, xi)))*np.cos(lss_t)*torch.ones(sum(dims))\n",
        "    grads = [grad[sum(dims[:i]):sum(dims[:i+1])] for i in range(n)]\n",
        "  elif algo == 'gnd':\n",
        "    xi = torch.cat([grad_L[i][i] for i in range(n)])\n",
        "    ham = torch.dot(xi, xi.detach())\n",
        "    grads = [get_gradient(ham, th[i]) for i in range(n)]\n",
        "  else: # GD\n",
        "    grads = [grad_L[i][i] for i in range(n)]\n",
        "\n",
        "  # Update theta\n",
        "  past_th = [th[i].clone() for i in range(n)]\n",
        "  with torch.no_grad():\n",
        "    for i in range(n):\n",
        "      th[i] -= alpha*grads[i]\n",
        "  return th, losses, past_th"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1rLMKEj0idp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown Game definitions.\n",
        "\n",
        "def market():\n",
        "  dims = [1, 1]\n",
        "  def Ls(th):\n",
        "    x, y = th\n",
        "    g = y**4/(1+x**2)-x**4/(1+y**2)\n",
        "    L_1 = x*y-x**2/2+x**6/6+g/4\n",
        "    L_2 = -x*y-y**2/2+y**6/6-g/4\n",
        "    return [L_1, L_2]\n",
        "  return dims, Ls\n",
        "\n",
        "def zerosum():\n",
        "  dims = [1, 1]\n",
        "  def Ls(th):\n",
        "    x, y = th\n",
        "    L_1 = x*y-x**2/2+y**2/2+x**4/4-y**4/4\n",
        "    return [L_1, -L_1]\n",
        "  return dims, Ls\n",
        "\n",
        "def market_with_min():\n",
        "  dims = [1, 1]\n",
        "  sig = 0.01\n",
        "  def Ls(th):\n",
        "    x, y = th\n",
        "    if x**2+y**2 >= sig**2:\n",
        "      f = (x**2+y**2-sig**2)/2\n",
        "    else:\n",
        "      f = (y**2-3*x**2)*(x**2+y**2-sig**2)/(2*sig**2)\n",
        "    g = y**4/(1+x**2)-x**4/(1+y**2)\n",
        "    L_1 = x**6/6-x**2+f+x*y+g/4\n",
        "    L_2 = y**6/6-f-x*y-g/4\n",
        "    return [L_1, L_2]\n",
        "  return dims, Ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6vrjepB7yWe",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown Plotting function.\n",
        "def plot_param(th, algo, start=0):\n",
        "  fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n",
        "  ax = ax.flatten()\n",
        "  for i, algo in enumerate(algos):\n",
        "    ax[i].plot(th_out[i, start:, 0], th_out[i, start:, 1],\n",
        "               sns.xkcd_rgb[\"prussian blue\"], lw=1.5)\n",
        "    ax[i].set_title(algo)\n",
        "    if i == 4 or i == 9:\n",
        "      ax[i].yaxis.tick_right()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wde5CX1qHHRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run each algorithm on a chosen game (market, zerosum or market_with_min)\n",
        "dims, Ls = zerosum()\n",
        "algos = ['gd', 'eg', 'omd', 'co', 'sga', 'la', 'lola', 'sos', 'cgd', 'lss']\n",
        "alpha = 0.02\n",
        "num_epochs = 3000\n",
        "\n",
        "th_out = np.zeros((len(algos), num_epochs, sum(dims)))\n",
        "for i, algo in enumerate(algos):\n",
        "  th = init_th(dims, std=1)\n",
        "  past_th = th.copy()\n",
        "  losses_out = np.zeros((num_epochs, len(dims)))\n",
        "  for k in range(num_epochs):\n",
        "    th_out[i, k] = [theta.clone().data.numpy() for theta in th]\n",
        "    th, losses, past_th = update_th(th, Ls, alpha, algo, lss_t=k, omd_th=past_th)\n",
        "    losses_out[k] = [loss for loss in losses]\n",
        "\n",
        "# Plot results from epoch=start onwards (for visual clarity)\n",
        "plot_param(th_out, algos, start=500)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}